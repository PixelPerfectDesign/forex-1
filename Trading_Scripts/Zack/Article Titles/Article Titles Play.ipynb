{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pybrain\n",
    "from pybrain.structure import FeedForwardNetwork\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "from pybrain.structure import FullConnection\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import random\n",
    "import ast\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import event\n",
    "import sqlite3\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import *\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import glob\n",
    "import pprint\n",
    "import dateutil.parser\n",
    "import pprint\n",
    "import re\n",
    "from sklearn import linear_model, datasets\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import gevent.monkey\n",
    "gevent.monkey.patch_socket()\n",
    "import numpy\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import event\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT\n",
    "import sqlite3\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import *\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import pprint\n",
    "import dateutil.parser\n",
    "import gevent\n",
    "import datetime\n",
    "import marshal\n",
    "\n",
    "import re, math, collections, itertools\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "mysql_url = \"mysql://forex:yummy4money@forex.c2ggnaqt6wye.us-west-1.rds.amazonaws.com/forex\"\n",
    "sqlite_url = 'sqlite:///database.db'\n",
    "db = create_engine(mysql_url, echo=False)\n",
    "session = sessionmaker()\n",
    "session.configure(bind=db)\n",
    "session = session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"select title, pub_date, section from articles_titles \n",
    "inner join tags_titles on articles_titles.article_id = tags_titles.article_id\n",
    "where tags_titles.category = 'world'\n",
    "limit %s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_count = 100000000\n",
    "query_formatted = query % row_count\n",
    "result = db.engine.execute(query_formatted)\n",
    "rows = []\n",
    "for row in result:\n",
    "    rows.append(row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in rows:\n",
    "    \n",
    "    country = item[2].strip(\"world/\").strip('\"')\n",
    "    #title = item[0].strip('\"').strip(\",\").strip(\":\").strip(\"'\").strip(\"?\").strip('\"')\n",
    "    title = item[0].decode('unicode_escape').encode('ascii','ignore').replace('\\n', '')\n",
    "    date = item[1]\n",
    "    data[country].append({'title': title, 'date': date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5176368"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for key, item in data.iteritems():\n",
    "    total += len(item)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['ussia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary of date to list of articles on that date\n",
    "from datetime import date\n",
    "ukraine = {}\n",
    "start_date = date(2005,1,1)\n",
    "for delta_day in range(4000) :\n",
    "    ukraine[ start_date+datetime.timedelta(delta_day) ] = []\n",
    "for item in data['ukraine'][20:]:\n",
    "    curdate = datetime.date( item['date'].year, item['date'].month, item['date'].day )\n",
    "    if curdate in ukraine :\n",
    "        ukraine[curdate].append(item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ukraine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-55e934b0444a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdelta_day\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mukraine\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'pos'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ukraine' is not defined"
     ]
    }
   ],
   "source": [
    "# calc values from article dictionary\n",
    "val=0\n",
    "for delta_day in range(4000) :\n",
    "    for title in ukraine[start_date+datetime.timedelta(delta_day)] :\n",
    "        sentiment = classifier.classify(extract_features(title.split()))\n",
    "        val += 1 if sentiment=='pos' else -1\n",
    "        #print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = []\n",
    "n = 100\n",
    "\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"china\"} for x in random.sample(data['china'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"japan\"} for x in random.sample(data['japan'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"india\"} for x in random.sample(data['india'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"iran\"} for x in random.sample(data['iran'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"turkey\"} for x in random.sample(data['turkey'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"ussia\"} for x in random.sample(data['ussia'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"iraq\"} for x in random.sample(data['iraq'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"yemen\"} for x in random.sample(data['yemen'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"canada\"} for x in random.sample(data['canada'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"germany\"} for x in random.sample(data['germany'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"spain\"} for x in random.sample(data['spain'], n)]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"sweden\"} for x in random.sample(data['sweden'], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pos = [{'title': item['title'], 'date': item['date'], 'country': item['country'], 'rating': \"pos\"} for item in countries]\n",
    "random.shuffle(pos)\n",
    "f = open('mycsvfile.csv','wb')\n",
    "w = csv.DictWriter(f,['country', 'date', 'title', 'rating'])\n",
    "w.writerows(pos)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pos = [{'title': item['title'], 'date': item['date'], 'country': item['country']} for item in countries]\n",
    "out_lines = []\n",
    "\n",
    "for x in range(len(countries) / 5 - 2):\n",
    "    out_lines.append([x['title'] for x in countries[5*x:5*x+5]])\n",
    "\n",
    "f = open('mycsvfile.csv','wb')\n",
    "w = csv.writer(f)\n",
    "w.writerows(out_lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"How relevant is this title towards political instability\"\n",
    "\"What is the sentiment of this title in regards to political instability?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-05d3b1fa5471>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'country'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'annotated.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countries' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "pos = [{'title': item['title'], 'date': item['date'], 'country': item['country'], 'rating': \"pos\"} for item in countries]\n",
    "random.shuffle(pos)\n",
    "f = open('annotated.csv','rb')\n",
    "w = csv.DictReader(f,['country', 'date', 'title', 'rating'])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<csv.DictReader instance at 0x7f0a75d1edd0>\n"
     ]
    }
   ],
   "source": [
    "in_data = []\n",
    "with open('annotated.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    print reader\n",
    "    for row in reader:\n",
    "        in_data.append(row)\n",
    "        \n",
    "for item in in_data:\n",
    "    item['rating'] =int(item['rating']) - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<csv.DictReader instance at 0x7f0a75b956c8>\n"
     ]
    }
   ],
   "source": [
    "in_data = []\n",
    "with open('mturk.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    print reader\n",
    "    for row in reader:\n",
    "        in_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'canada',\n",
       " 'date': '7/2/11 23:43',\n",
       " 'rating': 0,\n",
       " 'shit': 'pos',\n",
       " 'title': 'Duke and Duchess of Cambridge visit Canada - day three in pictures'}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -2,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -2,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['rating'] for x in in_data if int(x['rating']) < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 618 instances, test on 206 instances\n",
      "accuracy: 0.616504854369\n",
      "Most Informative Features\n",
      "                  attack = True              neg : pos    =      6.7 : 1.0\n",
      "                  Canada = True              pos : neg    =      5.7 : 1.0\n",
      "                   Iraqi = True              neg : pos    =      4.9 : 1.0\n",
      "                   found = True              neg : pos    =      4.9 : 1.0\n",
      "                 cables: = True              pos : neg    =      4.2 : 1.0\n",
      "                     off = True              neg : pos    =      4.0 : 1.0\n",
      "                   Syria = True              neg : pos    =      4.0 : 1.0\n",
      "                Pakistan = True              neg : pos    =      4.0 : 1.0\n",
      "                   fears = True              neg : pos    =      4.0 : 1.0\n",
      "                  Turkey = True              neg : pos    =      4.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    words = filter(lambda a: 'Yemen' not in a, words)\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = [x['title'] for x in in_data if int(x['rating']) < 0]\n",
    "posids = [x['title'] for x in in_data if int(x['rating']) >= 0]\n",
    "\n",
    "random.shuffle(negids)\n",
    "random.shuffle(posids)\n",
    " \n",
    "negfeats = [(word_feats(f.split(' ')), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(f.split(' ')), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 227 instances, test on 76 instances\n",
      "accuracy: 0.710526315789\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import *\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    words = filter(lambda a: 'Yemen' not in a, words)\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = [x['title'] for x in in_data if int(x['rating']) < 0]\n",
    "posids = [x['title'] for x in in_data if int(x['rating']) >= 0]\n",
    "\n",
    "random.shuffle(negids)\n",
    "random.shuffle(posids)\n",
    " \n",
    "negfeats = [(word_feats(f.split(' ')), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(f.split(' ')), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "classif = SklearnClassifier(LinearSVC())\n",
    "\n",
    "classifier = SklearnClassifier.train(classif, trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "#classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy is 0.719930555556\n",
      "stdev 0.0366618657421\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    #words = filter(lambda a: 'Yemen' not in a, words)\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "values = []\n",
    "n = 200\n",
    "\n",
    "for x in range(n):\n",
    "\n",
    "    negids = [x['title'] for x in in_data if int(x['rating']) < 0]\n",
    "    posids = [x['title'] for x in in_data if int(x['rating']) == 0]\n",
    "\n",
    "    random.shuffle(negids)\n",
    "    random.shuffle(posids)\n",
    "\n",
    "    negfeats = [(word_feats(f.split(' ')), 'neg') for f in negids]\n",
    "    posfeats = [(word_feats(f.split(' ')), 'pos') for f in posids]\n",
    "\n",
    "    negcutoff = len(negfeats)*3/4\n",
    "    poscutoff = len(posfeats)*3/4\n",
    "\n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from nltk.classify.scikitlearn import SklearnClassifier\n",
    "    \n",
    "    classif = SklearnClassifier(LinearSVC())\n",
    "    classifier = SklearnClassifier.train(classif, trainfeats)\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "\n",
    "    #print 'accuracy:', accuracy\n",
    "    values.append(accuracy)\n",
    "print \"mean accuracy is\", sum(values) / n\n",
    "print \"stdev\", np.std(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'war:': True, 'power': True, 'Saddam': True, 'to': True, 'in': True, 'path': True, 'The': True}, 'nega')\n"
     ]
    }
   ],
   "source": [
    "print negfeats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(word_feats(\"Duke and Duchess of Cambridge visit Canada - day three in pictures\".split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
