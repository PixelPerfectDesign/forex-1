{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pybrain\n",
    "from pybrain.structure import FeedForwardNetwork\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "from pybrain.structure import FullConnection\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import random\n",
    "import ast\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import event\n",
    "import sqlite3\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import *\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import glob\n",
    "import pprint\n",
    "import dateutil.parser\n",
    "import pprint\n",
    "import re\n",
    "from sklearn import linear_model, datasets\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import gevent.monkey\n",
    "gevent.monkey.patch_socket()\n",
    "import numpy\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import event\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT\n",
    "import sqlite3\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import *\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import pprint\n",
    "import dateutil.parser\n",
    "import gevent\n",
    "import datetime\n",
    "import marshal\n",
    "\n",
    "import re, math, collections, itertools\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "mysql_url = \"mysql://forex:yummy4money@forex.c2ggnaqt6wye.us-west-1.rds.amazonaws.com/forex\"\n",
    "sqlite_url = 'sqlite:///database.db'\n",
    "db = create_engine(mysql_url, echo=False)\n",
    "session = sessionmaker()\n",
    "session.configure(bind=db)\n",
    "session = session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"select title, pub_date, section from articles_titles \n",
    "inner join tags_titles on articles_titles.article_id = tags_titles.article_id\n",
    "where tags_titles.category = 'world'\n",
    "limit %s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_count = 100000000\n",
    "query_formatted = query % row_count\n",
    "result = db.engine.execute(query_formatted)\n",
    "rows = []\n",
    "for row in result:\n",
    "    rows.append(row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in rows:\n",
    "    \n",
    "    country = item[2].strip(\"world/\").strip('\"')\n",
    "    #title = item[0].strip('\"').strip(\",\").strip(\":\").strip(\"'\").strip(\"?\").strip('\"')\n",
    "    title = item[0].decode('unicode_escape').encode('ascii','ignore').replace('\\n', '')\n",
    "    date = item[1]\n",
    "    data[country].append({'title': title, 'date': date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for key, item in data.iteritems():\n",
    "    total += len(item)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary of date to list of articles on that date\n",
    "from datetime import date\n",
    "ukraine = {}\n",
    "start_date = date(2005,1,1)\n",
    "for delta_day in range(4000) :\n",
    "    ukraine[ start_date+datetime.timedelta(delta_day) ] = []\n",
    "for item in data['ukraine'][20:]:\n",
    "    curdate = datetime.date( item['date'].year, item['date'].month, item['date'].day )\n",
    "    if curdate in ukraine :\n",
    "        ukraine[curdate].append(item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ukraine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-55e934b0444a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdelta_day\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mukraine\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'pos'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ukraine' is not defined"
     ]
    }
   ],
   "source": [
    "# calc values from article dictionary\n",
    "val=0\n",
    "for delta_day in range(4000) :\n",
    "    for title in ukraine[start_date+datetime.timedelta(delta_day)] :\n",
    "        sentiment = classifier.classify(extract_features(title.split()))\n",
    "        val += 1 if sentiment=='pos' else -1\n",
    "        #print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = []\n",
    "n = 100\n",
    "\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"china\"} for x in data['china']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"japan\"} for x in data['japan']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"india\"} for x in data['india']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"iran\"} for x in data['iran']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"turkey\"} for x in data['turkey']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"ussia\"} for x in data['ussia']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"iraq\"} for x in data['iraq']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"yemen\"} for x in data['yemen']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"canada\"} for x in data['canada']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"germany\"} for x in data['germany']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"spain\"} for x in data['spain']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"bolivia\"} for x in data['bolivia']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"sweden\"} for x in data['sweden']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"malaysia\"} for x in data['malaysia']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"syria\"} for x in data['syria']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"sudan\"} for x in data['sudan']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"somalia\"} for x in data['somalia']]\n",
    "countries += [{\"title\": x['title'], \"date\": x['date'], \"country\": \"ghana\"} for x in data['ghana']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pos = [{'title': item['title'], 'date': item['date'], 'country': item['country'], 'rating': \"pos\"} for item in countries]\n",
    "random.shuffle(pos)\n",
    "f = open('mycsvfile.csv','wb')\n",
    "w = csv.DictWriter(f,['country', 'date', 'title', 'rating'])\n",
    "w.writerows(pos)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pos = [{'title': item['title'], 'date': item['date'], 'country': item['country']} for item in countries]\n",
    "\n",
    "#Write csv for importing to MTURK\n",
    "\n",
    "out_lines = []\n",
    "\n",
    "for x in range(len(countries) / 5 - 2):\n",
    "    out_lines.append([x['title'] for x in countries[5*x:5*x+5]])\n",
    "\n",
    "f = open('mycsvfile.csv','wb')\n",
    "w = csv.writer(f)\n",
    "w.writerows(out_lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"How relevant is this title towards political instability\"\n",
    "\"What is the sentiment of this title in regards to political instability?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-05d3b1fa5471>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'country'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'annotated.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countries' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "pos = [{'title': item['title'], 'date': item['date'], 'country': item['country'], 'rating': \"pos\"} for item in countries]\n",
    "random.shuffle(pos)\n",
    "f = open('annotated.csv','rb')\n",
    "w = csv.DictReader(f,['country', 'date', 'title', 'rating'])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# David annotating\n",
    "david_data = []\n",
    "with open('annotated.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        david_data.append(row)\n",
    "        \n",
    "for item in david_data:\n",
    "    item['rating'] = int(item['rating']) - 3\n",
    "    \n",
    "random.shuffle(david_data)\n",
    "train_data = david_data[:200]\n",
    "test_data = david_data[201:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<csv.DictReader instance at 0x7fcbf41650e0>\n"
     ]
    }
   ],
   "source": [
    "#FIrst Mturk attempt\n",
    "train_data = []\n",
    "with open('mturk.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    print reader\n",
    "    for row in reader:\n",
    "        train_data.append(row)\n",
    "        \n",
    "train_data = train_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "#second mturk attempt\n",
    "titles = defaultdict(list)\n",
    "with open('mturk3.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for x in range(1, 6):\n",
    "            titles[row[\"Input.HEADLINE_%s\" % x]].append(row[\"Answer.Q%sAnswer\" % x])\n",
    "            \n",
    "with open('mturk2.csv', 'rU') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for x in range(1, 6):\n",
    "            titles[row[\"Input.HEADLINE_%s\" % x]].append(row[\"Answer.Q%sAnswer\" % x])\n",
    "\n",
    "out = []\n",
    "for key, value in titles.iteritems():\n",
    "    try:\n",
    "        out.append([np.mean(np.array(value).astype(int)), np.std((np.array(value).astype(int))), key])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print len(out)\n",
    "filtered = filter(lambda x: x[1] < .5, sorted(out, key=lambda x: x[0]))\n",
    "print len(filtered)\n",
    "\n",
    "train_data = []\n",
    "for item in filtered:\n",
    "    train_data.append({'title': item[2], 'rating': item[0]})\n",
    "    \n",
    "    \n",
    "#train_data = train_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 204 instances, test on 42 instances\n",
      "accuracy: 0.547619047619\n",
      "Most Informative Features\n",
      "                      US = True              neg : pos    =      7.0 : 1.0\n",
      "                  threat = True              neg : pos    =      5.7 : 1.0\n",
      "                  attack = True              neg : pos    =      4.3 : 1.0\n",
      "                  police = True              neg : pos    =      3.7 : 1.0\n",
      "                    from = True              pos : neg    =      3.0 : 1.0\n",
      "                 Iranian = True              neg : pos    =      3.0 : 1.0\n",
      "                    over = True              neg : pos    =      2.7 : 1.0\n",
      "                      on = True              pos : neg    =      2.6 : 1.0\n",
      "              earthquake = True              pos : neg    =      2.3 : 1.0\n",
      "                 Beijing = True              pos : neg    =      2.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "trainfeats = process(train_data)\n",
    "testfeats = process(test_data)\n",
    "\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    words = filter(lambda a: 'Yemen' not in a, words)\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "\n",
    "\n",
    "def process(dataset, threshold):\n",
    "    \n",
    "    negids = [x['title'] for x in dataset if int(x['rating']) < threshold]\n",
    "    posids = [x['title'] for x in dataset if int(x['rating']) >= threshold]\n",
    "\n",
    "    random.shuffle(negids)\n",
    "    random.shuffle(posids)\n",
    "\n",
    "    negfeats = [(word_feats(f.split(' ')), 'neg') for f in negids]\n",
    "    \n",
    "    #Note the matching of lengths!!\n",
    "    posfeats = [(word_feats(f.split(' ')), 'pos') for f in posids][:len(negfeats)]\n",
    "    \n",
    "\n",
    "    \n",
    "    return negfeats + posfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy is 0.779195402299\n",
      "stdev 0.0244598133264\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "n = 200\n",
    "\n",
    "for x in range(n):\n",
    "    \n",
    "    #Play with things here\n",
    "    random.shuffle(david_data)\n",
    "    #train_data = david_data[:200]\n",
    "    test_data = david_data#[201:300]\n",
    "    \n",
    "    trainfeats = process(train_data, 0)\n",
    "    testfeats = process(test_data, 0)\n",
    "\n",
    "    classif = SklearnClassifier(LinearSVC())\n",
    "    classifier = SklearnClassifier.train(classif, trainfeats)\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    values.append(accuracy)\n",
    "\n",
    "print \"mean accuracy is\", sum(values) / n\n",
    "print \"stdev\", np.std(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Putin's\", 'military', 'exercises', 'are', 'more', 'than', 'a', 'game']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['title'].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'130': True,\n",
       " 'after': True,\n",
       " 'bombings': True,\n",
       " 'dead': True,\n",
       " 'leave': True,\n",
       " 'mosques': True,\n",
       " 'over': True,\n",
       " 'suicide': True,\n",
       " 'targeted': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfeats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for item in testfeats:\n",
    "    #print classifier.classify(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rating': -2.0, 'title': 'Death sentence for Chinese Christian sect trio'},\n",
       " {'rating': -2.0, 'title': 'China protester paralysed by beating'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Japans survivors: So many have been killed. Its hard to sleep. Unimaginable'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Japan tsunami: A shortage of petrol and food but many bodies to bury'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Spy plane crisis: An explicit threat - but only in Chinese'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Tornado rips through eastern Japan leaving dozens injured - video'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'After the earthquake, a tsunami of death and destruction in Japan'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Four charged with twin Bombay bombings'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'Taiwan president shot in election attack'},\n",
       " {'rating': -1.6666666666666667,\n",
       "  'title': 'China police station attack leaves several dead in Xinjiang'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
